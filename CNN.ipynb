{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0899b677-5e94-498f-8652-17842b857ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MuMu = pd.read_csv(\"data\\\\MuMu_dataset_multi-label.csv\")\n",
    "\n",
    "# Limpiar el dataset\n",
    "MuMu.dropna(subset=['amazon_id', 'genres'], inplace=True)\n",
    "MuMu = MuMu[['amazon_id', 'genres']]\n",
    "MuMu.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241f6a8-3e89-457f-a831-10c2e00d671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar la cantidad de géneros por registro\n",
    "MuMu['num_genres'] = MuMu['genres'].apply(lambda x: len(x.split(',')))\n",
    "\n",
    "# Filtrar los registros con más de 5 géneros\n",
    "MuMu_mas_de_5_generos = MuMu[MuMu['num_genres'] > 5]\n",
    "\n",
    "MuMu_mas_de_5_generos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424b854-ef81-49a2-89f2-af47497afcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la columna de géneros en una lista y limitar a 5 géneros como máximo por registro\n",
    "MuMu['genres'] = MuMu['genres'].apply(lambda x: ','.join(x.split(',')[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5727868c-9d15-4992-b028-6310ca4d6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el directorio donde se encuentran los archivos\n",
    "directory = \"data\\\\MUMU\"\n",
    "\n",
    "# Crear una lista para almacenar los nombres de los archivos sin la extensión\n",
    "file_names_without_extension = []\n",
    "\n",
    "# Iterar sobre todos los registros del dataset MuMu\n",
    "for index, row in MuMu.iterrows():\n",
    "    amazon_id = row['amazon_id']\n",
    "    filename = f\"{amazon_id}.jpg\"\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    \n",
    "    if not os.path.isfile(file_path):\n",
    "        # Si el archivo no existe, eliminar el registro del dataset\n",
    "        MuMu.drop(index, inplace=True)\n",
    "        print(f\"Registro eliminado: {amazon_id} (archivo {filename} no encontrado)\")\n",
    "    elif not filename.lower().endswith('.jpg'):\n",
    "        try:\n",
    "            # Si el archivo existe pero no es .jpg, eliminar el archivo y luego el registro del dataset\n",
    "            os.remove(file_path)\n",
    "            MuMu.drop(index, inplace=True)\n",
    "            print(f\"Registro eliminado: {amazon_id} (archivo {filename} es de otro formato)\")\n",
    "        except Exception as e:\n",
    "            print(f\"No se pudo eliminar {file_path}. Error: {e}\")\n",
    "    else:\n",
    "        # Guardar el nombre del archivo sin la extensión en la lista\n",
    "        file_name_without_extension = os.path.splitext(filename)[0]\n",
    "        file_names_without_extension.append(file_name_without_extension)\n",
    "\n",
    "df_deleted_files = pd.DataFrame(file_names_without_extension, columns=['amazon_id'])\n",
    "\n",
    "print(\"Archivos eliminados que no son .jpg:\", len(file_names_without_extension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d3accb-0c33-4d0b-9f00-52bb3232df38",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_freq = MuMu['genres'].apply(lambda s: str(s).split(',')).explode().value_counts().sort_values(ascending=False)\n",
    "top_20_labels = label_freq.head(20)\n",
    "\n",
    "style.use(\"fivethirtyeight\")\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.barplot(y=top_20_labels.index.values, x=top_20_labels, order=top_20_labels.index)\n",
    "plt.title(\"Label Frequency - Top 20\", fontsize=14)\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1f8847-a925-41fa-a36b-bd57ab64a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Obtener los 10 géneros más frecuentes\n",
    "top_10_labels = label_freq.head(10).index\n",
    "\n",
    "# Paso 2: Filtrar el dataset para obtener solo los registros que tienen exclusivamente uno de los géneros del top 10\n",
    "filtered_dfs = []\n",
    "for label in top_10_labels:\n",
    "    # Filtrar registros que tienen exactamente un género en el top 10\n",
    "    filtered_df = MuMu[MuMu['genres'].apply(lambda x: x.strip() == label)]\n",
    "    \n",
    "    # Seleccionar aleatoriamente hasta 400 registros si hay más de 400, sino toma todos\n",
    "    if len(filtered_df) > 1200:\n",
    "        filtered_df = filtered_df.sample(n=1200, random_state=42)\n",
    "    \n",
    "    filtered_dfs.append(filtered_df)\n",
    "\n",
    "# Paso 3: Concatenar los dataframes filtrados en un nuevo dataset\n",
    "MuMu = pd.concat(filtered_dfs).reset_index(drop=True)\n",
    "\n",
    "# Verificar el resultado final\n",
    "print(\"Distribución de géneros en el dataset filtrado:\")\n",
    "print(MuMu['genres'].value_counts())\n",
    "print(MuMu.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cafc0d-d0be-46be-8a4e-2949bac9e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = list(label_freq[label_freq<1000].index)\n",
    "print(\"We will be ignoring these rare labels:\", rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe8a5b3-63b3-4c92-a33e-b2183de7d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "MuMu['genres'] = MuMu['genres'].apply(lambda s: [l for l in str(s).split(',') if l not in rare])\n",
    "# Eliminar registros que quedan vacíos después del filtrado\n",
    "MuMu = MuMu[MuMu['genres'].apply(len) > 0]\n",
    "print(MuMu.sort_values(by='genres'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1150646-747b-4d87-b1a5-168ad3aad7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(MuMu['amazon_id'], MuMu['genres'], test_size=0.2, random_state=44)\n",
    "print(\"Number of albums for training: \", len(X_train))\n",
    "print(\"Number of albums for validation: \", len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283dc42c-01ac-4d7d-ae34-c367ee150809",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [os.path.join('data\\\\MUMU', str(f)+'.jpg') for f in X_train]\n",
    "X_val = [os.path.join('data\\\\MUMU', str(f)+'.jpg') for f in X_val]\n",
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0612a4d-9209-499f-b41b-c1b16e52f97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = list(y_train)\n",
    "y_val = list(y_val)\n",
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c95de-c544-4911-ba33-e62c6e8e71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs = 8\n",
    "ncols = 4\n",
    "nrows = nobs//ncols\n",
    "\n",
    "style.use(\"default\")\n",
    "plt.figure(figsize=(12,4*nrows))\n",
    "for i in range(nrows*ncols):\n",
    "    ax = plt.subplot(nrows, ncols, i+1)\n",
    "    img = Image.open(X_train[i])\n",
    "    plt.imshow(img)\n",
    "    plt.title(y_train[i], size=10)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51c0f88-6cbc-44f4-8923-c7f0944c3761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir la lista de géneros separada por comas a géneros individuales\n",
    "y_train_individual = [genre.split(',') for genres in y_train for genre in genres]\n",
    "\n",
    "print(\"Labels:\")\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(y_train_individual)\n",
    "\n",
    "N_LABELS = len(mlb.classes_)\n",
    "for (i, label) in enumerate(mlb.classes_):\n",
    "    print(\"{}. {}\".format(i, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cddf26-041d-40c5-9589-0f2117f53283",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_bin = mlb.transform(y_train)\n",
    "y_val_bin = mlb.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd2ff76-cd30-446b-a8f4-972fbab845f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(X_train[i], y_train_bin[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fef418-b368-4b6f-bc8f-5ef1aa4f8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c278990e-3e8d-490f-9830-6ab4345813d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_function(filename, label):\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
    "    image_resized = tf.image.resize(image_decoded, [IMG_SIZE, IMG_SIZE])\n",
    "    image_normalized = image_resized / 255.0\n",
    "    return image_normalized, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebab05d-3917-42cd-aa0c-f8af93a48060",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "SHUFFLE_BUFFER_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634add99-9691-49e2-919f-d7c6da3c8e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    dataset = dataset.map(parse_function, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    if is_training == True:\n",
    "        dataset = dataset.cache()\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "        \n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454ca2b6-5b8a-434d-8206-f609a545fd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_dataset(X_train, y_train_bin)\n",
    "val_ds = create_dataset(X_val, y_val_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2edfe1-e3ed-45e0-ae61-56695fbad3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, l in train_ds.take(1):\n",
    "    print(\"Shape of features array:\", f.numpy().shape)\n",
    "    print(\"Shape of labels array:\", l.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3093a9-3ee8-4a1d-a725-a1704bf67cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    bce_loss = keras.losses.BinaryCrossentropy(from_logits=False)(y_true, y_pred)\n",
    "    \n",
    "    is_all_zero_pred = tf.reduce_all(tf.equal(y_pred, 0), axis=-1)\n",
    "    penalization = tf.where(is_all_zero_pred, tf.ones_like(bce_loss) * 10.0, tf.zeros_like(bce_loss))\n",
    "    return bce_loss + penalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f43cc-a363-4bc8-964a-55e54da2a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.MobileNetV2(\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
    "x = scale_layer(inputs)\n",
    "\n",
    "x = base_model(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = keras.layers.Dense(N_LABELS, activation='sigmoid', name='output')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary(show_trainable=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 30\n",
    "print(\"Fitting the top layer of the model\")\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3527a33d-e62a-4daa-b836-fff7120584bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener predicciones del modelo en el conjunto de validación\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for x_batch, y_batch in val_ds:\n",
    "    y_true.append(y_batch.numpy())\n",
    "    y_pred.append(model.predict(x_batch))\n",
    "\n",
    "# Convertir las listas a arrays\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e12bf3-cce0-4881-bb30-878305574100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener predicciones y etiquetas verdaderas del conjunto de validación\n",
    "val_predictions = model.predict(val_ds)  # Predicciones del modelo\n",
    "val_labels = np.concatenate([y for x, y in val_ds], axis=0)  # Etiquetas verdaderas\n",
    "\n",
    "# Calcula la curva ROC y AUC para cada etiqueta\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(N_LABELS):\n",
    "    fpr[i], tpr[i], _ = roc_curve(val_labels[:, i], val_predictions[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Graficar las curvas ROC\n",
    "plt.figure(figsize=(12, 8))  # Aumenta el tamaño de la figura para más claridad\n",
    "for i in range(N_LABELS):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2, label=f'ROC curve for {label_names[i]} (area = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)  # Añade una cuadrícula para mejor visualización\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708de2b-cb40-40e1-95d0-cae69c04f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curves(history):    \n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    macro_f1 = history.history['macro_soft_f1']\n",
    "    val_macro_f1 = history.history['val_macro_soft_f1']\n",
    "    \n",
    "    epochs = len(loss)\n",
    "\n",
    "    style.use(\"bmh\")\n",
    "    plt.figure(figsize=(8, 8))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(range(1, epochs+1), loss, label='Training Loss')\n",
    "    plt.plot(range(1, epochs+1), val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(range(1, epochs+1), macro_f1, label='Training Macro F1-score')\n",
    "    plt.plot(range(1, epochs+1), val_macro_f1, label='Validation Macro F1-score')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.ylabel('Macro F1-score')\n",
    "    plt.title('Training and Validation Macro F1-score')\n",
    "    plt.xlabel('epoch')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return loss, val_loss, macro_f1, val_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44e0e5-03de-4289-83a6-c95024d292f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history['binary_accuracy'])\n",
    "print(history.history['loss'])\n",
    "print(history.history['val_binary_accuracy'])\n",
    "print(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e532e8f5-5acb-41bc-ba08-1968bdfe9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar las predicciones\n",
    "y_pred = model.predict(val_ds)\n",
    "\n",
    "# Convertir las probabilidades en etiquetas binarias\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Obtener las etiquetas verdaderas del conjunto de validación\n",
    "y_true = np.concatenate([y for x, y in val_ds], axis=0)\n",
    "\n",
    "# Calcula la matriz de confusión para cada etiqueta\n",
    "conf_matrix = confusion_matrix(y_true.argmax(axis=1), y_pred_binary.argmax(axis=1))\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Etiqueta Real\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e6a3d4-aaef-4c59-bc3d-8e632cf5365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "plt.plot(history.history['binary_accuracy'])\n",
    "plt.plot(history.history['val_binary_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45e059-2e27-4644-83dd-74844400b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def macro_soft_f1(y, y_hat):\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    y_hat = tf.cast(y_hat, tf.float32)\n",
    "    tp = tf.reduce_sum(y_hat * y, axis=0)\n",
    "    fp = tf.reduce_sum(y_hat * (1 - y), axis=0)\n",
    "    fn = tf.reduce_sum((1 - y_hat) * y, axis=0)\n",
    "    soft_f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    cost = 1 - soft_f1\n",
    "    macro_cost = tf.reduce_mean(cost)\n",
    "    return macro_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6b7df4-f134-4c81-bb72-8debc9573fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def macro_f1(y, y_hat, thresh=0.5):\n",
    "   \n",
    "    y_pred = tf.cast(tf.greater(y_hat, thresh), tf.float32)\n",
    "    tp = tf.cast(tf.math.count_nonzero(y_pred * y, axis=0), tf.float32)\n",
    "    fp = tf.cast(tf.math.count_nonzero(y_pred * (1 - y), axis=0), tf.float32)\n",
    "    fn = tf.cast(tf.math.count_nonzero((1 - y_pred) * y, axis=0), tf.float32)\n",
    "    f1 = 2*tp / (2*tp + fn + fp + 1e-16)\n",
    "    macro_f1 = tf.reduce_mean(f1)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0d2fd-1317-4854-97f0-21cc5ae02a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.MobileNetV2(\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False,\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "scale_layer = keras.layers.Rescaling(scale=1 / 127.5, offset=-1)\n",
    "x = scale_layer(inputs)\n",
    "\n",
    "x = base_model(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)\n",
    "outputs = keras.layers.Dense(N_LABELS, activation='sigmoid', name='output')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary(show_trainable=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=macro_soft_f1,\n",
    "    metrics=[macro_soft_f1],\n",
    ")\n",
    "epochs = 30\n",
    "print(\"Fitting the top layer of the model\")\n",
    "history = model.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c513fe-8a5e-462c-91fb-7b0b21b3a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "losses, val_losses, macro_f1s, val_macro_f1s = learning_curves(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
